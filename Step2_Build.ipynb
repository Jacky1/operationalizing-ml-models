{"cells":[{"cell_type":"markdown","source":["### Operationalizing ML Models\nJohn Hoff  \nMachine Learning Architect  \njhoff@productiveedge.com\n# Step 2: Building the Model\n![Step 1: Prepare](https://drive.google.com/uc?export=view&id=1LzKYjwhjddy6IvNlrieIhA62d2Cvk07w)\n\nThis step will create an ML Pipeline based model using Spark MLlib.  The model will then be trained, scored, and saved to an MLflow experiment run.\n\n_Please Note: The \"Run All\" command is safe to run on this notebook._"],"metadata":{}},{"cell_type":"code","source":["import mlflow\nimport mlflow.spark\n\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import SQLTransformer\nfrom pyspark.ml.feature import Imputer\nfrom pyspark.ml.feature import StandardScaler\nfrom pyspark.ml.feature import StringIndexer\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.classification import RandomForestClassifier\n\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":2},{"cell_type":"code","source":["seed = 1023"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":3},{"cell_type":"markdown","source":["A new MLflow run will be created if one is not currently active.  The MLflow run is used to track the performance of the model and to serialize the model for later use."],"metadata":{}},{"cell_type":"code","source":["if not mlflow.active_run():\n  mlflow.start_run()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":5},{"cell_type":"markdown","source":["## Part 1: Load the Dataset\nThe training data is loaded from Spark SQL and split into a 75%/25% split for training and testing."],"metadata":{}},{"cell_type":"code","source":["data = spark.sql(\"select * from bank_marketing_training\")\ntraining_data, testing_data = data.randomSplit([0.75, 0.25], seed=seed)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":7},{"cell_type":"markdown","source":["## Part 2: Create the ML Pipeline\nAdditional documentation can be found on the Spark ML Piplines here: https://spark.apache.org/docs/latest/ml-pipeline.html\n\nThe high-level goal here is to create a model that accepts human-readable input parameters, thus making the model far more accessible.  Secondary goals are to have the pipeline robust enough to handle missing and unknown inputs and be flexible enough for reuse in later steps."],"metadata":{}},{"cell_type":"code","source":["preprocessing_pipeline = Pipeline(stages=[\n  \n  # This is the cleanest method I have found to date for ensuring that all incoming\n  # numeric feature values are the floating point numbers that Spark expects.\n  SQLTransformer(statement=\"\"\"\n  select\n    *,\n    cast(age as double) as age__double,\n    cast(duration as double) as duration__double,\n    cast(campaign as double) as campaign__double,\n    cast(pdays as double) as pdays__double,\n    cast(previous as double) as previous__double,\n    cast(`emp.var.rate` as double) as emp_var_rate__double,\n    cast(`cons.price.idx` as double) as cons_price_idx__double,\n    cast(`cons.conf.idx` as double) as cons_conf_idx__double,\n    cast(euribor3m as double) as euribor3m__double,\n    cast(`nr.employed` as double) as nr_employed__double\n  from __THIS__\n  \"\"\"),\n  \n  # The handling of each feature is explicitly handled. With the immutability of the underlying\n  # data, each transformer extends the original dataset by adding columns.  In this case, it\n  # that some mental tracking is required to ensure input features are properly connected to\n  # the desired output features.\n  Imputer(inputCols=['age__double'], outputCols=['age__imputed'], strategy='median'),\n  StringIndexer(inputCol='job', outputCol='job__index', handleInvalid='keep'),\n  StringIndexer(inputCol='marital', outputCol='marital__index', handleInvalid='keep'),\n  StringIndexer(inputCol='education', outputCol='education__index', handleInvalid='keep'),\n  StringIndexer(inputCol='default', outputCol='default__index', handleInvalid='keep'),\n  StringIndexer(inputCol='housing', outputCol='housing__index', handleInvalid='keep'),\n  StringIndexer(inputCol='loan', outputCol='loan__index', handleInvalid='keep'),\n  StringIndexer(inputCol='contact', outputCol='contact__index', handleInvalid='keep'),\n  StringIndexer(inputCol='month', outputCol='month__index', handleInvalid='keep'),\n  StringIndexer(inputCol='day_of_week', outputCol='day_of_week__index', handleInvalid='keep'),\n  Imputer(inputCols=['duration__double'], outputCols=['duration__imputed'], strategy='median'),\n  Imputer(inputCols=['campaign__double'], outputCols=['campaign__imputed'], strategy='median'),\n  Imputer(inputCols=['pdays__double'], outputCols=['pdays__imputed'], strategy='median'),\n  Imputer(inputCols=['previous__double'], outputCols=['previous__imputed'], strategy='median'),\n  StringIndexer(inputCol='poutcome', outputCol='poutcome__index', handleInvalid='keep'),\n  Imputer(inputCols=['emp_var_rate__double'], outputCols=['emp_var_rate__imputed'], strategy='median'),\n  Imputer(inputCols=['cons_price_idx__double'], outputCols=['cons_price_idx__imputed'], strategy='median'),\n  Imputer(inputCols=['cons_conf_idx__double'], outputCols=['cons_conf_idx__imputed'], strategy='median'),\n  Imputer(inputCols=['euribor3m__double'], outputCols=['euribor3m__imputed'], strategy='median'),\n  Imputer(inputCols=['nr_employed__double'], outputCols=['nr_employed__imputed'], strategy='median'),\n  \n  # With all feature engineering completed, a single features vector is assembled to be fed into\n  # the estimator in the pipeline.\n  VectorAssembler(\n    inputCols=[\n      'age__imputed',\n      'job__index',\n      'marital__index',\n      'education__index',\n      'default__index',\n      'housing__index',\n      'loan__index',\n      'contact__index',\n      'month__index',\n      'day_of_week__index',\n      'duration__imputed',\n      'campaign__imputed',\n      'pdays__imputed',\n      'previous__imputed',\n      'poutcome__index',\n      'emp_var_rate__imputed',\n      'cons_price_idx__imputed',\n      'cons_conf_idx__imputed',\n      'euribor3m__imputed',\n      'nr_employed__imputed',\n    ],\n    outputCol='features'\n  ),\n])\n\n# The target pipeline-based model is then created using the preprocessing pipeline as the first stage.\npipeline = Pipeline(stages=[\n  preprocessing_pipeline,\n  StringIndexer(inputCol='y', outputCol='label'),\n  RandomForestClassifier(featuresCol='features', labelCol='label', seed=1023)\n])"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":9},{"cell_type":"markdown","source":["## Part 3: Train the Model"],"metadata":{}},{"cell_type":"code","source":["model = pipeline.fit(training_data)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":11},{"cell_type":"markdown","source":["## Part 4: Score the Model"],"metadata":{}},{"cell_type":"code","source":["predictions = model.transform(testing_data)\n\nmulticlass_evaluator = MulticlassClassificationEvaluator()\nbinary_evaluator = BinaryClassificationEvaluator()\n\naccuracy = multiclass_evaluator.evaluate(predictions, {multiclass_evaluator.metricName:'accuracy'})\nprint('Accuracy: %s' % accuracy)\nmlflow.log_metric('accuracy', accuracy)\n\nf1 = multiclass_evaluator.evaluate(predictions, {multiclass_evaluator.metricName:'f1'})\nprint('F1: %s' % f1)\nmlflow.log_metric('f1', f1)\n\nprecision = multiclass_evaluator.evaluate(predictions, {multiclass_evaluator.metricName:'weightedPrecision'})\nprint('Precision: %s' % precision)\nmlflow.log_metric('precision', precision)\n\nrecall = multiclass_evaluator.evaluate(predictions, {multiclass_evaluator.metricName:'weightedRecall'})\nprint('Recall: %s' % recall)\nmlflow.log_metric('recall', recall)\n\nroc_auc = binary_evaluator.evaluate(predictions)\nprint('ROC AUC: %s' % roc_auc)\nmlflow.log_metric('auc', roc_auc)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Accuracy: 0.9060100166944908\nF1: 0.8848966052582152\nPrecision: 0.89120127730648\nRecall: 0.9060100166944908\nROC AUC: 0.9322880054432626\n</div>"]}}],"execution_count":13},{"cell_type":"markdown","source":["## Part 5: Save the Model"],"metadata":{}},{"cell_type":"code","source":["mlflow.spark.log_model(model, 'spark_model')\nmlflow.end_run()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":15}],"metadata":{"name":"Step2_Build","notebookId":4046375874530638},"nbformat":4,"nbformat_minor":0}
