{"cells":[{"cell_type":"markdown","source":["### Operationalizing ML Models\nJohn Hoff  \nMachine Learning Architect  \njhoff@productiveedge.com\n# Step 6: Analyze Model Usage\n![Step 1: Prepare](https://drive.google.com/uc?export=view&id=1V7ASlRjSUjYAH4LBiJiQtQraTwW4FScx)\n\nThis step will use the ML Pipeline created in Step 2 to perform a drift analysis.\n\n_Please Note: The \"Run All\" command is safe to run on this notebook._"],"metadata":{}},{"cell_type":"code","source":["import mlflow\nimport mlflow.spark\n\nimport numpy as np\nimport pandas as pd\n\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import SQLTransformer\nfrom pyspark.ml.feature import Imputer\nfrom pyspark.ml.feature import StandardScaler\nfrom pyspark.ml.feature import StringIndexer\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.classification import RandomForestClassifier\n\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.mllib.evaluation import MulticlassMetrics\n\nimport re"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":2},{"cell_type":"code","source":["seed=1023"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":3},{"cell_type":"code","source":["if not mlflow.active_run():\n  mlflow.start_run()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":4},{"cell_type":"markdown","source":["## Part 1: Combining the Training Dataset and Usage Dataset\nAs the training dataset and usage dataset are combined, an attribute is added to indicate whether the record is a live observation or not.  The subsequent machine learning training will utilize this as the target.  The resulting model will be used to predict whether a given observation belongs in the training set or the usage set."],"metadata":{}},{"cell_type":"code","source":["data = spark.sql(\"\"\"\nselect *, 'no' as live_observation from bank_marketing_training\nunion\nselect *, 'yes' as live_observation from bank_marketing_usage\n\"\"\")\ntraining_data, testing_data = data.randomSplit([0.75, 0.25], seed=seed)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":6},{"cell_type":"markdown","source":["## Part 2: Reusing our ML Pipeline in a New Model\nThe ML pipeline created in Step 2 is being reused here.  In the wild this would typcally be done using a Python library."],"metadata":{}},{"cell_type":"code","source":["preprocessing_pipeline = Pipeline(stages=[\n  \n  # This is the cleanest method I have found to date for ensuring that all incoming\n  # numeric feature values are the floating point numbers that Spark expects.\n  SQLTransformer(statement=\"\"\"\n  select\n    *,\n    cast(age as double) as age__double,\n    cast(duration as double) as duration__double,\n    cast(campaign as double) as campaign__double,\n    cast(pdays as double) as pdays__double,\n    cast(previous as double) as previous__double,\n    cast(`emp.var.rate` as double) as emp_var_rate__double,\n    cast(`cons.price.idx` as double) as cons_price_idx__double,\n    cast(`cons.conf.idx` as double) as cons_conf_idx__double,\n    cast(euribor3m as double) as euribor3m__double,\n    cast(`nr.employed` as double) as nr_employed__double\n  from __THIS__\n  \"\"\"),\n  \n  # The handling of each feature is explicitly handled. With the immutability of the underlying\n  # data, each transformer extends the original dataset by adding columns.  In this case, it\n  # that some mental tracking is required to ensure input features are properly connected to\n  # the desired output features.\n  Imputer(inputCols=['age__double'], outputCols=['age__imputed'], strategy='median'),\n  StringIndexer(inputCol='job', outputCol='job__index', handleInvalid='keep'),\n  StringIndexer(inputCol='marital', outputCol='marital__index', handleInvalid='keep'),\n  StringIndexer(inputCol='education', outputCol='education__index', handleInvalid='keep'),\n  StringIndexer(inputCol='default', outputCol='default__index', handleInvalid='keep'),\n  StringIndexer(inputCol='housing', outputCol='housing__index', handleInvalid='keep'),\n  StringIndexer(inputCol='loan', outputCol='loan__index', handleInvalid='keep'),\n  StringIndexer(inputCol='contact', outputCol='contact__index', handleInvalid='keep'),\n  StringIndexer(inputCol='month', outputCol='month__index', handleInvalid='keep'),\n  StringIndexer(inputCol='day_of_week', outputCol='day_of_week__index', handleInvalid='keep'),\n  Imputer(inputCols=['duration__double'], outputCols=['duration__imputed'], strategy='median'),\n  Imputer(inputCols=['campaign__double'], outputCols=['campaign__imputed'], strategy='median'),\n  Imputer(inputCols=['pdays__double'], outputCols=['pdays__imputed'], strategy='median'),\n  Imputer(inputCols=['previous__double'], outputCols=['previous__imputed'], strategy='median'),\n  StringIndexer(inputCol='poutcome', outputCol='poutcome__index', handleInvalid='keep'),\n  Imputer(inputCols=['emp_var_rate__double'], outputCols=['emp_var_rate__imputed'], strategy='median'),\n  Imputer(inputCols=['cons_price_idx__double'], outputCols=['cons_price_idx__imputed'], strategy='median'),\n  Imputer(inputCols=['cons_conf_idx__double'], outputCols=['cons_conf_idx__imputed'], strategy='median'),\n  Imputer(inputCols=['euribor3m__double'], outputCols=['euribor3m__imputed'], strategy='median'),\n  Imputer(inputCols=['nr_employed__double'], outputCols=['nr_employed__imputed'], strategy='median'),\n  \n  # With all feature engineering completed, a single features vector is assembled to be fed into\n  # the estimator in the pipeline.\n  VectorAssembler(\n    inputCols=[\n      'age__imputed',\n      'job__index',\n      'marital__index',\n      'education__index',\n      'default__index',\n      'housing__index',\n      'loan__index',\n      'contact__index',\n      'month__index',\n      'day_of_week__index',\n      'duration__imputed',\n      'campaign__imputed',\n      'pdays__imputed',\n      'previous__imputed',\n      'poutcome__index',\n      'emp_var_rate__imputed',\n      'cons_price_idx__imputed',\n      'cons_conf_idx__imputed',\n      'euribor3m__imputed',\n      'nr_employed__imputed',\n    ],\n    outputCol='features'\n  ),\n])\n\nrandomForest = RandomForestClassifier(featuresCol='features', labelCol='label', seed=1023)\n\npipeline = Pipeline(stages=[\n  preprocessing_pipeline,\n  StringIndexer(inputCol='live_observation', outputCol='label'),\n  randomForest,\n])"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":8},{"cell_type":"markdown","source":["## Part 3: Train our Model"],"metadata":{}},{"cell_type":"code","source":["model = pipeline.fit(training_data)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":10},{"cell_type":"markdown","source":["## Part 4: Score our Model"],"metadata":{}},{"cell_type":"code","source":["predictions = model.transform(testing_data)\n\nmulticlass_evaluator = MulticlassClassificationEvaluator()\nbinary_evaluator = BinaryClassificationEvaluator()\n\naccuracy = multiclass_evaluator.evaluate(predictions, {multiclass_evaluator.metricName:'accuracy'})\nprint('Accuracy: %s' % accuracy)\nmlflow.log_metric('accuracy', accuracy)\n\nf1 = multiclass_evaluator.evaluate(predictions, {multiclass_evaluator.metricName:'f1'})\nprint('F1: %s' % f1)\nmlflow.log_metric('f1', f1)\n\nprecision = multiclass_evaluator.evaluate(predictions, {multiclass_evaluator.metricName:'weightedPrecision'})\nprint('Precision: %s' % precision)\nmlflow.log_metric('precision', precision)\n\nrecall = multiclass_evaluator.evaluate(predictions, {multiclass_evaluator.metricName:'weightedRecall'})\nprint('Recall: %s' % recall)\nmlflow.log_metric('recall', recall)\n\nroc_auc = binary_evaluator.evaluate(predictions)\nprint('ROC AUC: %s' % roc_auc)\nmlflow.log_metric('auc', roc_auc)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Accuracy: 0.6363281631060385\nF1: 0.5928295714848165\nPrecision: 0.635867877611495\nRecall: 0.6363281631060385\nROC AUC: 0.6495455702142\n</div>"]}}],"execution_count":12},{"cell_type":"markdown","source":["## Part 5: Test for Drift\nThe Mathews Correlation Coefficient is being used to measure the quality of the classification.  It is the correlation between the predicted and actual target values.  It can be interpreted using the standard correlation interpretations.\n\n+ < 0.2 is considered uncorrelated\n+ < 0.4 is a weak correlation\n+ < 0.6 is a moderate correlation\n+ \\>= 0.8 is a strong correlation\n\nDetermining the correlation threshold to associate with drift will depend very much on the context.  I feel that a weak correlation warrants investigating further and a moderate correlation or stronger should be considered very troublesome."],"metadata":{}},{"cell_type":"code","source":["metrics = MulticlassMetrics(predictions.select('prediction', 'label').rdd)\nconfusionMatrix = metrics.confusionMatrix().toArray()\nTP = confusionMatrix[1][1]\nTN = confusionMatrix[0][0]\nFN = confusionMatrix[1][0]\nFP = confusionMatrix[0][1]\nMCC = ((TP*TN)-(FP*FN))/np.sqrt((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN))\n\nprint('MCC: %s' % MCC)\nmlflow.log_metric('MCC', MCC) "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">MCC: 0.208715775772\n</div>"]}}],"execution_count":14},{"cell_type":"markdown","source":["## Part 6: Show Features Important to Drift\nUnpacking the important features from the random forest isn't as straight-forward as it needs to be.  With some quick traversal of the pipeline, I can pull out both the feature importances and the input columns being assembled into the feature column.  Sorted in descending order, the features that are causing the drift are easy to spot."],"metadata":{}},{"cell_type":"code","source":["# This function is looking for stages of the pipeline that have outputCol='features'\n# This stage allows for resolving the incoming columns to the final features\n# In our case, this will be the VectorAssembler\ndef find_feature_transformer(pipeline):\n  for stage in pipeline.stages:\n    if hasattr(stage, 'outputCol') and stage.getOutputCol() == 'features':\n      return stage\n    if hasattr(stage, 'stages'):\n      found = find_feature_transformer(stage)\n      if found:\n        return found\n  return None\n\n# This function is looking for stages of the pipeline that have featureImportances\n# In our case, this will be the RandomForestClassifier\ndef find_importance_estimator(pipeline):\n  for stage in pipeline.stages:\n    if hasattr(stage, 'featureImportances'):\n      return stage\n    if hasattr(stage, 'stages'):\n      found = find_importance_estimator(stage)\n      if found:\n        return found\n  return None\n\nfeature_transformer = find_feature_transformer(model)\nimportance_estimator = find_importance_estimator(model)\n\n# We iterate over the features and find the corresponding input column\nimportances_data = []\nfor index, importance in enumerate(importance_estimator.featureImportances):\n  importances_data.append([\n    # I am pulling off the suffixes I have been adding in the pipeline.  You will\n    # have to juggle things a bit more when using one-hot encoding for any categories.\n    re.sub(r'__.*', '', feature_transformer.getInputCols()[index]),\n    importance\n  ])\n  \n# To make things easier to print, we are creating a pandas dataframe.\nimportances = pd.DataFrame(importances_data, columns=['feature', 'importance'])\nimportances.sort('importance', ascending=False, inplace=True)\ndisplay(importances)\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>feature</th><th>importance</th></tr></thead><tbody><tr><td>marital</td><td>0.3084461489509242</td></tr><tr><td>job</td><td>0.2887046113627158</td></tr><tr><td>education</td><td>0.19210112186607203</td></tr><tr><td>age</td><td>0.045885686929312774</td></tr><tr><td>emp_var_rate</td><td>0.03832618208872269</td></tr><tr><td>cons_price_idx</td><td>0.03577935591768235</td></tr><tr><td>nr_employed</td><td>0.03413147174775455</td></tr><tr><td>cons_conf_idx</td><td>0.013949843248127166</td></tr><tr><td>duration</td><td>0.006529959129871715</td></tr><tr><td>default</td><td>0.006490057623435581</td></tr><tr><td>euribor3m</td><td>0.00512025841832452</td></tr><tr><td>month</td><td>0.004854056316607474</td></tr><tr><td>previous</td><td>0.0036773649522886843</td></tr><tr><td>campaign</td><td>0.0036328598171303343</td></tr><tr><td>pdays</td><td>0.003571728531129786</td></tr><tr><td>day_of_week</td><td>0.003479224303561621</td></tr><tr><td>loan</td><td>0.0027770157204942965</td></tr><tr><td>poutcome</td><td>0.0012648500079006839</td></tr><tr><td>contact</td><td>8.232765109410471E-4</td></tr><tr><td>housing</td><td>4.5492655700282616E-4</td></tr></tbody></table></div>"]}}],"execution_count":16},{"cell_type":"code","source":["mlflow.end_run()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":17}],"metadata":{"name":"Step6_Analyze","notebookId":4046375874530650},"nbformat":4,"nbformat_minor":0}
